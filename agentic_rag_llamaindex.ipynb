{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components in LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Query engine for RAG\n",
    "\n",
    "#### Setting up the persona database \n",
    "i will be using personas from the https://huggingface.co/datasets/dvilasuero/finepersonas-v0.1-tiny. This dataset contains 5K personas that will be attending the party!\n",
    "\n",
    "Let's load the dataset and store it as files in the data directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      + c:\\Users\\loicsteve.fohoue\\OneDrive - Virgo Facilities\\Bureau\\LlamaIndexAgents\\.venv\\Scripts\\python.exe C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9\\vendored-meson\\meson\\meson.py setup C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9 C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9\\.mesonpy-g50r5edt -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9\\.mesonpy-g50r5edt\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9\n",
      "      Build dir: C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9\\.mesonpy-g50r5edt\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      Running `cl /?` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      Running `cc --version` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      Running `gcc --version` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      Running `clang --version` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] Le fichier spÃ©cifiÃ© est introuvable\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\loicsteve.fohoue\\AppData\\Local\\Temp\\pip-install-hke2s2xa\\numpy_45c98572cdf84680b301507e2bdfe9b9\\.mesonpy-g50r5edt\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index datasets llama-index-callbacks-arize-phoenix llama-index-vector-stores-chroma llama-index-llms-huggingface-api -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since dvilasuero/finepersonas-v0.1-tiny couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\loicsteve.fohoue\\.cache\\huggingface\\datasets\\dvilasuero___finepersonas-v0.1-tiny\\default\\0.0.0\\877c402c4434d631b5055853bc50ba93fbdf9c12 (last modified on Tue Apr 15 11:51:38 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "for i, persona in enumerate(dataset):\n",
    "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and embedding persona documents\n",
    "\n",
    "We will use the SimpleDirectoryReader to load the persona descriptions from the data directory. This will return a list of Document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Using cached llama_index-0.12.30-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Using cached llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama_index)\n",
      "  Using cached llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.30 (from llama_index)\n",
      "  Using cached llama_index_core-0.12.30-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Using cached llama_index_llms_openai-0.3.35-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Using cached llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama_index)\n",
      "  Using cached llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama_index)\n",
      "  Using cached llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
      "  Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama_index) (3.9.1)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading openai-1.74.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama_index) (2.0.40)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (3.11.16)\n",
      "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached banks-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (2024.12.0)\n",
      "Collecting httpx (from llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (2.2.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (11.2.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (2.11.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.30->llama_index) (1.17.2)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud-0.1.18-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.2.3)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.12-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama_index) (1.19.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (1.7.2)\n",
      "Collecting jinja2 (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (4.3.7)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.1.31)\n",
      "Collecting anyio (from httpx->llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.13.0,>=0.12.30->llama_index)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama_index) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.30->llama_index) (0.14.0)\n",
      "Collecting llama-cloud-services>=0.6.12 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.12-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index)\n",
      "  Downloading jiter-0.9.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama_index) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.30->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama_index) (3.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.2)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.12->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama_index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\loicsteve.fohoue\\onedrive - virgo facilities\\bureau\\llamaindexagents\\.venv\\lib\\site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama_index) (3.0.2)\n",
      "Downloading llama_index-0.12.30-py3-none-any.whl (7.0 kB)\n",
      "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
      "Using cached llama_index_core-0.12.30-py3-none-any.whl (1.6 MB)\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_llms_openai-0.3.35-py3-none-any.whl (23 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached banks-2.1.1-py3-none-any.whl (28 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading llama_cloud-0.1.18-py3-none-any.whl (253 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Downloading llama_parse-0.6.12-py3-none-any.whl (4.9 kB)\n",
      "Downloading openai-1.74.0-py3-none-any.whl (644 kB)\n",
      "   ---------------------------------------- 0.0/644.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 644.8/644.8 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp313-cp313-win_amd64.whl (204 kB)\n",
      "Downloading llama_cloud_services-0.6.12-py3-none-any.whl (36 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: striprtf, soupsieve, python-dotenv, pypdf, jiter, jinja2, httpcore, distro, deprecated, anyio, httpx, dataclasses-json, beautifulsoup4, openai, llama-cloud, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "Successfully installed anyio-4.9.0 banks-2.1.1 beautifulsoup4-4.13.3 dataclasses-json-0.6.7 deprecated-1.2.18 distro-1.9.0 httpcore-1.0.8 httpx-0.28.1 jinja2-3.1.6 jiter-0.9.0 llama-cloud-0.1.18 llama-cloud-services-0.6.12 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.30 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.35 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.12 llama_index-0.12.30 openai-1.74.0 pypdf-5.4.0 python-dotenv-1.1.0 soupsieve-2.6 striprtf-0.0.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of Document objects, we can use the IngestionPipeline to create nodes from the documents and prepare them for the QueryEngine. We will use the SentenceSplitter to split the documents into smaller chunks and the HuggingFaceInferenceAPIEmbedding to embed the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loicsteve.fohoue\\OneDrive - Virgo Facilities\\Bureau\\LlamaIndexAgents\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\loicsteve.fohoue\\AppData\\Local\\llama_index\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
